{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "349DCCCB343F4B649A23AFB0E1822FAA",
    "mdEditEnable": false
   },
   "source": [
    "# Logistic Regression\n",
    "逻辑回归模型是一种常用的分类模型，它通过sigmoid或者softmax函数，将函数值映射到(0, 1)区间内，从而实现对样本的分类。在这个小作业中，你需要实现：\n",
    "1. 二分类和多分类两种逻辑回归模型\n",
    "2. 分别含有 L1 和 L2 两种正则项的损失函数，并计算对应的梯度\n",
    "3. 权重参数W的更新\n",
    "4. 比较不同的学习率对损失函数和分类器性能的影响\n",
    "5. 比较不同的正则项参数对于分类器性能的影响"
   ]
  },
  {
   "metadata": {
    "id": "2A7C0A404A2648F4B4D37BD564BF077E",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "81849DCA00F1484AA4EFCB7C8BA3CBDC",
    "mdEditEnable": false
   },
   "source": [
    "## 一、二分类逻辑回归：\n",
    "### 1.1数据集介绍\n",
    "这个任务中使用的数据集是手写数字集MNIST，它有50000个训练样本和10000个测试样本，共10个类别。在二分类任务上，我们对MNIST数据集进行了一个采样，抽取了数据集中的‘5’和‘3’对应的样本作为二分类的正负样本，共得到10842个训练样本，1784个测试样本，其中正负样本数量均相同。为了让大家对于这个数据集有一个更直观的认识，我们从正负样本中各抽取了8个样例进行了可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false,
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "7ED97409753E411AAE7237C565E7E95A",
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 16 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/7ED97409753E411AAE7237C565E7E95A/q76redtaac.png\">"
     },
     "transient": {}
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data1(path):\n",
    "    # load all MNIST data\n",
    "    fd = open(os.path.join(path, 'train-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    train_X_all = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n",
    "    fd = open(os.path.join(path, 'train-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    train_Y_all = loaded[8:].reshape(60000).astype(np.float)\n",
    "    fd = open(os.path.join(path, 't10k-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    test_X_all = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n",
    "    fd = open(os.path.join(path, 't10k-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    test_Y_all = loaded[8:].reshape(10000).astype(np.float)\n",
    "\n",
    "    #subsample data\n",
    "    train_idxs_df = pd.read_csv(os.path.join(path, 'train_indices.csv'))\n",
    "    test_idxs_df = pd.read_csv(os.path.join(path, 'test_indices.csv'))\n",
    "    pos_train_indices = train_idxs_df['pos_train_indices'].tolist()\n",
    "    neg_train_indices = train_idxs_df['neg_train_indices'].tolist()\n",
    "    pos_test_indices = test_idxs_df['pos_test_indices'].tolist()\n",
    "    neg_test_indices = test_idxs_df['neg_test_indices'].tolist()\n",
    "    train_Y_all[pos_train_indices] = 1\n",
    "    train_Y_all[neg_train_indices] = 0\n",
    "    test_Y_all[pos_test_indices] = 1\n",
    "    test_Y_all[neg_test_indices] = 0\n",
    "    train_indices = np.append(pos_train_indices, neg_train_indices)\n",
    "    test_indices = np.append(pos_test_indices, neg_test_indices)\n",
    "    train_X = train_X_all[train_indices]\n",
    "    train_Y = train_Y_all[train_indices]\n",
    "    test_X = test_X_all[test_indices]\n",
    "    test_Y = test_Y_all[test_indices]\n",
    "\n",
    "    #visualiza data\n",
    "    sample_num = 8\n",
    "    pos_sample_indices = np.random.choice(pos_train_indices, sample_num, replace=False)\n",
    "    neg_sample_indices = np.random.choice(neg_train_indices, sample_num, replace=False)\n",
    "    for i, idx in enumerate(pos_sample_indices):\n",
    "        plt_idx = i + 1\n",
    "        plt.subplot(2, sample_num, plt_idx)\n",
    "        plt.imshow(train_X_all[idx, :, :, :].reshape((28, 28)), cmap=plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Positive')\n",
    "\n",
    "    for i, idx in enumerate(neg_sample_indices):\n",
    "        plt_idx = sample_num + i + 1\n",
    "        plt.subplot(2, sample_num, plt_idx)\n",
    "        plt.imshow(train_X_all[idx, :, :, :].reshape((28, 28)), cmap=plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Negative')\n",
    "    \n",
    "    # reshaple into rows and normaliza\n",
    "    train_X = train_X.reshape((train_X.shape[0], -1))\n",
    "    test_X = test_X.reshape((test_X.shape[0], -1))\n",
    "    mean_image = np.mean(train_X, axis=0)\n",
    "    train_X = train_X - mean_image\n",
    "    test_X = test_X - mean_image\n",
    "\n",
    "    # add a bias columu into X\n",
    "    train_X = np.hstack([train_X, np.ones((train_X.shape[0], 1))])\n",
    "    test_X = np.hstack([test_X, np.ones((test_X.shape[0], 1))])\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_data1('/home/kesci/input/MNIST_dataset4284')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "0ADF94CCCE824695A9814D1730001A8D",
    "mdEditEnable": false
   },
   "source": "### 1.2逻辑回归模型\n在这一部分中你需要完成以下内容：\n1. train函数中权重的更新\n2. L1和L2两种正则化的损失函数及对应梯度的计算\n3. predict函数中的预测类别的计算"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "C0AAD39A300446338D60432B301DF402",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": "def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nclass LinearRegression1(object):\n    def __init__(self):\n        self.W = None\n    \n    \n    def train(self, X, Y, display, learning_rate=1e-3, reg=1e-5, reg_type='L2', num_iters=2000, batch_size=128):\n        num_train, feat_dim = X.shape\n        self.W = 0.001 * np.random.randn(feat_dim)\n        loss_history = []\n        for i in range(num_iters):\n            batch_indices = np.random.choice(num_train, batch_size, replace=True)\n            X_batch = X[batch_indices]\n            Y_batch = Y[batch_indices]\n            if reg_type == 'L1':\n                loss, grad = self.l1_loss(X_batch, Y_batch, reg)\n            else:\n                loss, grad = self.l2_loss(X_batch, Y_batch, reg)\n            loss_history.append(loss)\n            \n            # Todo 1\n            #lr = learning_rate / (i // 800 + 1)\n            #self.W -= lr * grad\n            self.W -= learning_rate * grad\n            \n            if display and i % 100 == 0:\n                print(\"In iteration {}/{} , the loss is {}\".format(i, num_iters, loss))\n        return loss_history\n\n\n    def loss_grad(self, X, Y, reg):\n        num, feat_dim = X.shape\n        \n        loss = 0\n        grad = np.zeros(feat_dim)\n        for i in range(num):\n            z = sigmoid(np.sum(self.W * X[i]))\n            loss += -Y[i] * math.log(z) - (1 - Y[i]) * math.log(1 - z)\n            grad += (z - Y[i]) * X[i]\n        \n        return loss / num, grad / num\n    \n    \n    def l1_loss(self, X, Y, reg):\n        # Todo 2\n        loss, grad = self.loss_grad(X, Y, reg)\n        \n        loss = loss + reg * np.sum(abs(self.W))\n        grad = grad + reg\n        \n        return loss, grad\n\n\n    def l2_loss(self, X, Y, reg):\n        # Todo 3\n        loss, grad = self.loss_grad(X, Y, reg)\n        \n        loss = loss + reg * np.sum(self.W * self.W)\n        grad = grad + 2 * reg * self.W\n        \n        return loss, grad\n\n\n    def predict(self, X, threshold=0.5):\n        # Todo 4\n        num, feat_dim = X.shape\n        Y_pred = np.zeros(num)\n\n        for i in range(num):\n            val = sigmoid(np.sum(self.W * X[i]))\n            if val >= threshold:\n                Y_pred[i] = 1\n            else:\n                Y_pred[i] = 0\n        \n        return Y_pred\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "C860E4A6D2084E68B890076E8105D4C5",
    "mdEditEnable": false
   },
   "source": "### 1.3 训练模型实例\n在这一部分，你不需要完成任何代码，你可以通过这一部分验证你上面实现的LogisticRegression1的代码是否实现正确。\n\n***Answer:\n无论是采用 L1 正则化还是 L2 正则化，最终的 accuracy 均保持在 94%~96%。***"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false,
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "8A4053C3BFC742D1A2A0326917E2B4C9",
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "text": "In iteration 0/2000 , the loss is 1.0085522003106717\nIn iteration 100/2000 , the loss is 0.16307978926765507\nIn iteration 200/2000 , the loss is 0.15338794582094234\nIn iteration 300/2000 , the loss is 0.17525618256538808\nIn iteration 400/2000 , the loss is 0.17715761989785897\nIn iteration 500/2000 , the loss is 0.10806885608443047\nIn iteration 600/2000 , the loss is 0.10650489191878605\nIn iteration 700/2000 , the loss is 0.12226732465605153\nIn iteration 800/2000 , the loss is 0.15981368249994374\nIn iteration 900/2000 , the loss is 0.1339793853593213\nIn iteration 1000/2000 , the loss is 0.131951880483782\nIn iteration 1100/2000 , the loss is 0.09361016842325245\nIn iteration 1200/2000 , the loss is 0.14187617762424212\nIn iteration 1300/2000 , the loss is 0.18638983126306052\nIn iteration 1400/2000 , the loss is 0.09144666113489389\nIn iteration 1500/2000 , the loss is 0.08538753642875357\nIn iteration 1600/2000 , the loss is 0.1170663814536412\nIn iteration 1700/2000 , the loss is 0.14958617905400046\nIn iteration 1800/2000 , the loss is 0.11709754342909828\nIn iteration 1900/2000 , the loss is 0.08172314581879764\nThe Accuracy is 0.9602017937219731\n\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/8A4053C3BFC742D1A2A0326917E2B4C9/q76revljqe.png\">"
     },
     "transient": {}
    }
   ],
   "source": "lr_param = 1.5e-6\nreg_param = 0.01\n\nmodel = LinearRegression1()\nloss_history = model.train(X_train, Y_train, True, lr_param, reg_param, 'L2')\npred = model.predict(X_test)\nacc = np.mean(pred == Y_test)\nprint(\"The Accuracy is {}\\n\".format(acc))\nx = range(len(loss_history))\nplt.plot(x, loss_history, label='Loss')\nplt.legend()\nplt.xlabel('Iteration Num')\nplt.ylabel('Loss')\nplt.show()\nW = model.W"
  },
  {
   "metadata": {
    "id": "27A95F6A3315482A8EF1F4DFE3A0025E",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "mdEditEnable": false
   },
   "cell_type": "markdown",
   "source": "### 1.4 学习率和Loss函数、模型性能的关系\n因为学习率和正则化参数都是超参数，在一般的训练过程中，我们没办法直接优化，所以我们一般会将训练集细分成训练集和验证集，然后通过模型在验证集上的表现选择一个最优的超参数，再将它对应的最优的模型应用到测试集中。\n在这一部分你需要完成以下内容：\n1. 尝试多种不同的学习率\n2. 储存学习率对应的损失函数值到L1_loss和L2_loss中（我们对损失函数值进行了20步平均化处理）。\n3. 储存学习率对应的**在验证集上**的正确率到L1_lr_val_acc和L2_lr_val_acc中\n\n#### 注意：\n因为已有代码中L1_loss，L1_lr_val_acc都是数组，在可视化的过程中我们需要学习率和它们相对应，比如learning_rates[0]对应的loss和validation accuracy应该储存在数组index为0的位置\n\n#### 拓展：\n在这个部分中采取的损失函数都是定值，如果你有时间的话，可以尝试根据迭代轮数改变学习率，并比较不变的学习率和变化的学习率对于模型性能的影响。"
  },
  {
   "metadata": {
    "id": "29B73EBF7A1A49A7AB8EF67379778334",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "text": "L1 1e-05 0.9594283079760259\nL1 7e-06 0.9557399723374828\nL1 4e-06 0.9543568464730291\nL1 1e-06 0.9474412171507607\nL1 7e-07 0.9432918395573997\nL1 3e-07 0.9377593360995851\nL1 1e-07 0.9294605809128631\nL2 1e-05 0.9571230982019364\nL2 7e-06 0.954817888427847\nL2 4e-06 0.9580451821115722\nL2 1e-06 0.9474412171507607\nL2 7e-07 0.9483633010603965\nL2 3e-07 0.9409866297833103\nL2 1e-07 0.9266943291839558\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/29B73EBF7A1A49A7AB8EF67379778334/q76qyrht4s.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/29B73EBF7A1A49A7AB8EF67379778334/q76qysioue.png\">"
     },
     "transient": {}
    }
   ],
   "source": "reg = 0.01\nreg_types = ['L1', 'L2']\nL1_loss = []\nL2_loss = []\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\nL1_lr_val_acc = []\nL2_lr_val_acc = []\n\n# Todo 5:\nlearning_rates = [1e-5, 7e-6, 4e-6, 1e-6, 7e-7, 3e-7, 1e-7]\nfor type in range(2):\n    for i in range(len(learning_rates)):\n        model = LinearRegression1()\n        learning_rate = learning_rates[i]\n        loss_history = model.train(X_train, Y_train, False, learning_rate, reg, reg_types[type])\n        accuracy = np.mean(model.predict(X_val) == Y_val)\n        if type == 0:\n            L1_loss.append(loss_history)\n            L1_lr_val_acc.append(accuracy)\n        else:\n            L2_loss.append(loss_history)\n            L2_lr_val_acc.append(accuracy)\n        print(reg_types[type], learning_rate, accuracy)\n\n#visulize the relationship between lr and loss\nfor i, lr in enumerate(learning_rates):\n    L1_loss_label = str(lr) + 'L1'\n    L2_loss_label = str(lr) + 'L2'\n    L1_loss_i = L1_loss[i]\n    L2_loss_i = L2_loss[i]\n    ave_L1_loss = np.zeros_like(L1_loss_i)\n    ave_L2_loss = np.zeros_like(L2_loss_i)\n    ave_step = 20\n    for j in range(len(L1_loss_i)):\n        if j < ave_step:\n            ave_L1_loss[j] = np.mean(L1_loss_i[0: j + 1])\n            ave_L2_loss[j] = np.mean(L2_loss_i[0: j + 1])\n        else:\n            ave_L1_loss[j] = np.mean(L1_loss_i[j - ave_step + 1: j + 1])    \n            ave_L2_loss[j] = np.mean(L2_loss_i[j - ave_step + 1: j + 1])\n    x = range(len(L1_loss_i))\n    plt.plot(x, ave_L1_loss, label=L1_loss_label)\n    plt.plot(x, ave_L2_loss, label=L2_loss_label)\n    \nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Loss')\nplt.show()\n\n#visulize the relationship between lr and accuracy\nx = range(len(learning_rates))\nplt.plot(x, L1_lr_val_acc, label='L1_val_acc')\nplt.plot(x, L2_lr_val_acc, label='L2_val_acc')\nplt.xticks(x, learning_rates)\nplt.margins(0.08)\nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Validation Accuracy')\nplt.show()",
   "execution_count": 81
  },
  {
   "metadata": {
    "id": "DA6289D7D8FF43C1A3257E3FC684E856",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "mdEditEnable": false
   },
   "cell_type": "markdown",
   "source": "#### Question1: 学习率和损失函数的变化、模型性能之间分别有什么关系？\n\nAnswer1:\n当学习率比较大时(`learning_rate >= 5e-5`)，在学习的迭代过程中会出现 `self.W * X[i]` 很大的情况，导致 `Sigmoid` 得到的函数值 `z` 趋近于 `1`，使得 `log(1-z)` 出现错误，因此学习率不能设置太大。\n\n同时可以看到学习率在 `7e-6 ~ 7e-7` 之间，模型准确率均能保持在 `95%` 左右。当学习率继续增加时，准确率开始下降，此时增大迭代次数可在一定程度上提高准确率。\n\n可以看到，在 `learning_rate >= 4e-6` 时，损失函数下降情况基本一致，当学习率逐渐减小时，学习率下降稍微减慢，且最后不收敛到同一值。\n\n此外，似乎在学习率大一些的时候，L1 正则化与 L2 正则化差别不大。当学习率比较小的时候，L2 正则化准确率比 L1 正则化准确率更低。\n\n**尝试了一下让学习率随迭代次数而减小……不过没什么效果，所以觉得还是固定学习率好。**"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "5CBDD6A9235E466482D04B6E7D0C6D5F",
    "mdEditEnable": false
   },
   "source": "### 1.5 正则项与模型性能\n在这一部分中，你需要完成以下内容：\n1. 尝试多个正则化参数的值\n2. 储存对应的在**验证集上**的正确率到L1_reg_val_acc和L2_reg_val_acc中\n3. 通过验证集X_val和Y_val选择最优的正则化超参数，并储存最优正则化参数和对应模型\n\n已有的代码会画出正则化参数和验证集上正确率的关系图，并计算最优的模型在测试集上的正确率。\n\n#### 注意：\n和上面学习率一样，L1_reg_val_acc的存储也需要和正则化参数值对应。"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "69F1B83300954853826A6383607380B2",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "text": "L1 0.001 0.9632564841498559\nL1 0.005 0.9567723342939481\nL1 0.01 0.9567723342939481\nL1 0.015 0.9553314121037464\nL1 0.02 0.9596541786743515\nL1 0.025 0.9596541786743515\nL1 0.03 0.9618155619596542\nL1 0.035 0.9582132564841499\nL1 0.04 0.9582132564841499\nL2 0.001 0.9531700288184438\nL2 0.005 0.957492795389049\nL2 0.01 0.9589337175792507\nL2 0.015 0.9589337175792507\nL2 0.02 0.9582132564841499\nL2 0.025 0.9582132564841499\nL2 0.03 0.9589337175792507\nL2 0.035 0.9589337175792507\nL2 0.04 0.9589337175792507\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/69F1B83300954853826A6383607380B2/q76r49s5sc.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "The Accuracy with L1 regularization parameter 0.001 is 0.9602017937219731\n\nThe Accuracy with L2 regularization parameter 0.01 is 0.952914798206278\n\n",
     "name": "stdout"
    }
   ],
   "source": "learning_rate = 1.5e-6\nreg_types = ['L1', 'L2']\nL1_reg_val_acc = []\nL2_reg_val_acc = []\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\nbest_L1_model = None\nbest_L2_model = None\nbest_L1_reg = 0\nbest_L2_reg = 0\n\n# Todo 6:\nregs = [0.001, 0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040]\nfor type in range(2):\n    best_accuracy = 0\n    for i in range(len(regs)):\n        model = LinearRegression1()\n        reg = regs[i]\n        loss_history = model.train(X_train, Y_train, False, learning_rate, reg, reg_types[type])\n        accuracy = np.mean(model.predict(X_val) == Y_val)\n        if type == 0:\n            L1_reg_val_acc.append(accuracy)\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_L1_reg = reg\n                best_L1_model = model\n        else:\n            L2_reg_val_acc.append(accuracy)\n            if (accuracy > best_accuracy):\n                best_accuracy = accuracy\n                best_L2_reg = reg\n                best_L2_model = model\n        print(reg_types[type], reg, accuracy)\n\n#visuliza the relation of regularization parameter and validation accuracy\nx = range(len(regs))\nplt.plot(x, L1_reg_val_acc, label='L1_val_acc')\nplt.plot(x, L2_reg_val_acc, label='L2_val_acc')\nplt.xticks(x, regs)\nplt.margins(0.08)\nplt.legend()\nplt.xlabel('high-parameter reg')\nplt.ylabel('Validation Accuracy')\nplt.show()\n\n#Compute the performance of best model on the test set\nL1_pred = best_L1_model.predict(X_test)\nL1_acc = np.mean(L1_pred == Y_test)\nprint(\"The Accuracy with L1 regularization parameter {} is {}\\n\".format(best_L1_reg, L1_acc))\nL2_pred = best_L2_model.predict(X_test)\nL2_acc = np.mean(L2_pred == Y_test)\nprint(\"The Accuracy with L2 regularization parameter {} is {}\\n\".format(best_L2_reg, L2_acc))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "FE5B019BDA934812B195721468F03A19",
    "mdEditEnable": false
   },
   "source": "## 二、多分类逻辑回归"
  },
  {
   "metadata": {
    "id": "CDA14DC4417C433280B811EB721C78AB",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "mdEditEnable": false
   },
   "cell_type": "markdown",
   "source": "### 2.1 加载数据集"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "2435572FAB2F49589F97188F6F3E00F3",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 80 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/2435572FAB2F49589F97188F6F3E00F3/q77l8ubj5i.png\">"
     },
     "transient": {}
    }
   ],
   "source": "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n\ndef load_data2(path):\n    # load all MNIST data\n    fd = open(os.path.join(path, 'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    train_X = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n    fd = open(os.path.join(path, 'train-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    train_Y = loaded[8:].reshape(60000).astype(np.float)\n    fd = open(os.path.join(path, 't10k-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    test_X = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n    fd = open(os.path.join(path, 't10k-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    test_Y = loaded[8:].reshape(10000).astype(np.float)\n\n    #visualiza data\n    sample_num = 8\n    num_classes = 10\n    for y in range(num_classes):\n        idxs = np.flatnonzero(train_Y == y)\n        idxs = np.random.choice(idxs, sample_num, replace=False)\n        for i, idx in enumerate(idxs):\n            plt_idx = i * num_classes + y + 1\n            plt.subplot(sample_num, num_classes, plt_idx)\n            plt.imshow(train_X[idx, :, :, :].reshape((28,28)),cmap=plt.cm.gray)\n            plt.axis('off')\n            if i == 0:\n                plt.title(y)\n    plt.show()\n\n    # reshaple into rows and normaliza\n    train_X = train_X.reshape((train_X.shape[0], -1))\n    test_X = test_X.reshape((test_X.shape[0], -1))\n    mean_image = np.mean(train_X, axis=0)\n    train_X = train_X - mean_image\n    test_X = test_X - mean_image\n\n    # add a bias columu into X\n    train_X = np.hstack([train_X, np.ones((train_X.shape[0], 1))])\n    test_X = np.hstack([test_X, np.ones((test_X.shape[0], 1))])\n    train_Y = train_Y.astype(np.int32)\n    test_Y = test_Y.astype(np.int32)\n    return train_X, train_Y, test_X, test_Y\n\n\nX_train, Y_train, X_test, Y_test = load_data2('/home/kesci/input/MNIST_dataset4284')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "CDE7A7DC8F9542EC93B98D91D6C0AA01",
    "mdEditEnable": false
   },
   "source": "### 2.2逻辑回归模型\n在这一部分中你需要完成与二分类逻辑回归相同的任务。"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "6A55B828E6AF46D88C07A902A8D51A51",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": "class LinearRegression2(object):\n    def __init__(self):\n        self.W = None\n    \n    def train(self, X, Y, display, learning_rate=1e-3, reg=1e-5, reg_type='L2', num_iters=2000,\n              batch_size=128):\n        num_train, feat_dim = X.shape\n        num_classes = 10\n        self.W = 0.001 * np.random.randn(feat_dim, num_classes).transpose()\n        loss_history = []\n        for i in range(num_iters):\n            batch_indices = np.random.choice(num_train, batch_size, replace=True)\n            X_batch = X[batch_indices]\n            Y_batch = Y[batch_indices]\n            if reg_type == 'L1':\n                loss, grad = self.l1_loss(X_batch, Y_batch, reg)\n            else:\n                loss, grad = self.l2_loss(X_batch, Y_batch, reg)\n            loss_history.append(loss)\n            \n            # Todo 1\n            self.W -= learning_rate * grad\n       \n            if display and i % 100 == 0:\n                print(\"In iteration {}/{} , the loss is {}\".format(i, num_iters, loss))\n        return loss_history\n\n    def loss_grad(self, X, Y, reg):\n        data_num, feat_dim = X.shape\n        class_num = self.W.shape[0]\n        \n        loss = 0\n        grad = np.zeros([class_num, feat_dim])\n        for data in range(data_num):\n            sum = 0\n            \n            exp_sum = np.exp(np.sum(X[data] * self.W, axis=1))\n            sum = exp_sum.sum()\n            loss += -np.log(exp_sum[Y[data]] / sum)\n            \n            for i in range(class_num):\n                p = exp_sum[i] / sum\n                if i == Y[data]:\n                    p -= 1\n                grad[i] += X[data] * p\n        \n        return loss / data_num, grad / data_num\n    \n    \n    def l1_loss(self,X, Y, reg):\n        # Todo 2\n        loss, grad = self.loss_grad(X, Y, reg)\n        \n        loss = loss + reg * abs(self.W).sum()\n        grad = grad + reg\n        \n        return loss, grad\n    \n    def l2_loss(self, X, Y, reg):\n        # Todo 3\n        loss, grad = self.loss_grad(X, Y, reg)\n        \n        loss = loss + reg * np.sum(self.W * self.W)\n        grad = grad + 2 * reg * self.W\n        \n        return loss, grad\n\n    def predict(self, X):\n        data_num, feat_dim = X.shape\n        class_num = self.W.shape[0]\n        Y_pred = np.zeros(data_num)\n        \n        for data in range(data_num):\n            max_p = 0\n            pos = 0\n            \n            exp_sum = np.exp(np.sum(self.W * X[data], axis=1))\n            Y_pred[data] = np.argmax(exp_sum)\n        \n        return Y_pred\n"
  },
  {
   "metadata": {
    "id": "9026A07B5DFC4D94BC46D06D92150869",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "357C61F2396B4616979B175BBDD0DE12",
    "mdEditEnable": false
   },
   "source": "### 2.3 训练模型样例\n在这一部分，你不需要完成任何代码，你可以通过这一部分验证你上面实现的LogisticRegression1的代码是否实现正确。"
  },
  {
   "metadata": {
    "id": "BF68315494B84FB2B401C86CB8788A4A",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "text": "In iteration 0/2000 , the loss is 7.74026280634094\nIn iteration 100/2000 , the loss is 1.6200678095793086\nIn iteration 200/2000 , the loss is 0.841270304352612\nIn iteration 300/2000 , the loss is 0.7669157691315339\nIn iteration 400/2000 , the loss is 0.7022173791691593\nIn iteration 500/2000 , the loss is 0.7651952244146731\nIn iteration 600/2000 , the loss is 0.7167420677259189\nIn iteration 700/2000 , the loss is 0.526422186100991\nIn iteration 800/2000 , the loss is 0.6015692372041385\nIn iteration 900/2000 , the loss is 0.6709375233420287\nIn iteration 1000/2000 , the loss is 0.5904602781961209\nIn iteration 1100/2000 , the loss is 0.575234514885548\nIn iteration 1200/2000 , the loss is 0.7388522504798021\nIn iteration 1300/2000 , the loss is 0.6164285600924444\nIn iteration 1400/2000 , the loss is 0.6561681887744997\nIn iteration 1500/2000 , the loss is 0.6192726692834813\nIn iteration 1600/2000 , the loss is 0.5933476173942047\nIn iteration 1700/2000 , the loss is 0.6515525313829451\nIn iteration 1800/2000 , the loss is 0.5181743048829983\nIn iteration 1900/2000 , the loss is 0.627379023429087\nThe Accuracy is 0.8938\n\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9x68zz.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "reg = 1000\n0\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9xgxcf.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "1\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9xf7t3.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "2\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9x62w9.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "3\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9xw6it.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "4\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9xg4r9.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "5\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9x9y0v.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "6\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9ybb13.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "7\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9ymgm.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "8\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9ylp24.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "9\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q77l9ybizx.png\">"
     },
     "transient": {}
    }
   ],
   "source": "lr_param = 7e-6\nreg_param = 500 # for visualize\n# reg_param = 0.01\nmodel = LinearRegression2()\nloss_history = model.train(X_train, Y_train, True, lr_param, reg_param, 'L2')\npred = model.predict(X_test)\nacc = np.mean(pred == Y_test)\nprint(\"The Accuracy is {}\\n\".format(acc))\nx = range(len(loss_history))\nplt.plot(x, loss_history, label='Loss')\nplt.legend()\nplt.xlabel('Iteration Num')\nplt.ylabel('Loss')\nplt.show()\n\nprint(\"reg =\", reg)\nW = model.W\nfor digit in range(10):\n    w = np.reshape(np.delete(W[digit], -1), (28, -1))\n    w_min = np.min(w)\n    w_max = np.max(w)\n    w = (w - w_min) / (w_max - w_min) * 255.0\n    print(digit)\n    plt.imshow(w, cmap=plt.cm.gray)\n    # plt.imshow(w)\n    plt.axis('off')\n    plt.show()\n",
   "execution_count": 36
  },
  {
   "metadata": {
    "id": "729A6A93708047EE8A92A46AEB87CCA7",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "mdEditEnable": false
   },
   "cell_type": "markdown",
   "source": "### 2.4 学习率与损失函数、模型性能的关系\n因为学习率和正则化参数都是超参数，在一般的训练过程中，我们没办法直接优化，所以我们一般会将训练集细分成训练集和验证集，然后通过模型在验证集上的表现选择一个最优的超参数，再将它对应的最优的模型应用到测试集中。\n在这一部分你需要完成以下内容：\n1. 尝试多种不同的学习率\n2. 储存学习率对应的损失函数值到L1_loss和L2_loss中（我们对损失函数值进行了20步平均化处理）。\n3. 储存学习率对应的**在验证集上**的正确率到L1_lr_val_acc和L2_lr_val_acc中\n\n#### 注意：\n因为已有代码中L1_loss，L1_lr_val_acc都是数组，在可视化的过程中我们需要学习率和它们相对应，比如learning_rates[0]对应的loss和validation accuracy应该储存在数组index为0的位置\n\n#### 拓展：\n在这个部分中采取的损失函数都是定值，如果你有时间的话，可以尝试根据迭代轮数改变学习率，并比较不变的学习率和变化的学习率对于模型性能的影响。"
  },
  {
   "metadata": {
    "id": "5A6CB337C471493B8DE3A3F868E49186",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "text": "L1 7e-05 0.9028125\nL1 4e-05 0.9083333333333333\nL1 1e-05 0.909375\nL1 7e-06 0.9117708333333333\nL1 4e-06 0.9066666666666666\nL1 1e-06 0.8851041666666667\nL1 7e-07 0.879375\nL2 7e-05 0.9\nL2 4e-05 0.9104166666666667\nL2 1e-05 0.911875\nL2 7e-06 0.9102083333333333\nL2 4e-06 0.9057291666666667\nL2 1e-06 0.8870833333333333\nL2 7e-07 0.8760416666666667\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/5A6CB337C471493B8DE3A3F868E49186/q76rsk4qi6.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/5A6CB337C471493B8DE3A3F868E49186/q76rsmw39z.png\">"
     },
     "transient": {}
    }
   ],
   "source": "reg = 0.01\nreg_types = ['L1', 'L2']\nL1_loss = []\nL2_loss = []\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\nL1_lr_val_acc = []\nL2_lr_val_acc = []\n\n# Todo 5:\nlearning_rates = [7e-5, 4e-5, 1e-5, 7e-6, 4e-6, 1e-6, 7e-7]\nfor type in range(2):\n    for i in range(len(learning_rates)):\n        model = LinearRegression2()\n        learning_rate = learning_rates[i]\n        loss_history = model.train(X_train, Y_train, False, learning_rate, reg, reg_types[type])\n        accuracy = np.mean(model.predict(X_val) == Y_val)\n        if type == 0:\n            L1_loss.append(loss_history)\n            L1_lr_val_acc.append(accuracy)\n        else:\n            L2_loss.append(loss_history)\n            L2_lr_val_acc.append(accuracy)\n        print(reg_types[type], learning_rate, accuracy)\n\n#visulize the relationship between lr and loss\nfor i, lr in enumerate(learning_rates):\n    L1_loss_label = str(lr) + 'L1'\n    L2_loss_label = str(lr) + 'L2'\n    L1_loss_i = L1_loss[i]\n    L2_loss_i = L2_loss[i]\n    ave_L1_loss = np.zeros_like(L1_loss_i)\n    ave_L2_loss = np.zeros_like(L2_loss_i)\n    ave_step = 20\n    for j in range(len(L1_loss_i)):\n        if j < ave_step:\n            ave_L1_loss[j] = np.mean(L1_loss_i[0: j + 1])\n            ave_L2_loss[j] = np.mean(L2_loss_i[0: j + 1])\n        else:\n            ave_L1_loss[j] = np.mean(L1_loss_i[j - ave_step + 1: j + 1])    \n            ave_L2_loss[j] = np.mean(L2_loss_i[j - ave_step + 1: j + 1])\n    x = range(len(L1_loss_i))\n    plt.plot(x, ave_L1_loss, label=L1_loss_label)\n    plt.plot(x, ave_L2_loss, label=L2_loss_label)\n    \nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Loss')\nplt.show()\n\n#visulize the relationship between lr and accuracy\nx = range(len(learning_rates))\nplt.plot(x, L1_lr_val_acc, label='L1_val_acc')\nplt.plot(x, L2_lr_val_acc, label='L2_val_acc')\nplt.xticks(x, learning_rates)\nplt.margins(0.08)\nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Validation Accuracy')\nplt.show()",
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "4A4B01F04B1241E089EE29390FDE9D2D",
    "mdEditEnable": false
   },
   "source": "### 2.5 正则项与模型性能\n在这一部分中，你需要完成以下内容：\n1. 尝试多个正则化参数的值\n2. 储存对应的在**验证集上**的正确率到L1_reg_val_acc和L2_reg_val_acc中\n3. 通过验证集X_val和Y_val选择最优的正则化超参数，并储存最优正则化参数和对应模型\n\n已有的代码会画出正则化参数和验证集上正确率的关系图，并计算最优的模型在测试集上的正确率。\n\n#### 注意：\n和上面学习率一样，L1_reg_val_acc的存储也需要和正则化参数值对应。"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false,
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "A37EA8B9B3BE427EA3B674AFC033EB84",
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "text": "L1 0.001 0.9072916666666667\nL1 0.01 0.9061197916666667\nL1 0.1 0.9088541666666666\nL1 1 0.91015625\n",
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n",
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": "L1 10 0.10013020833333333\nL1 100 0.10013020833333333\nL1 1000 0.10013020833333333\nL2 0.001 0.910546875\nL2 0.01 0.908984375\nL2 0.1 0.9100260416666667\nL2 1 0.9098958333333333\nL2 10 0.90859375\nL2 100 0.9015625\nL2 1000 0.875\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/A37EA8B9B3BE427EA3B674AFC033EB84/q77kydgldm.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "stream",
     "text": "The Accuracy with L1 regularization parameter 1 is 0.911\n\nThe Accuracy with L1 regularization parameter 0.001 is 0.9123\n\n",
     "name": "stdout"
    }
   ],
   "source": "learning_rate = 7e-6\nreg_types = ['L1', 'L2']\nL1_reg_val_acc = []\nL2_reg_val_acc = []\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\nbest_L1_model = None\nbest_L2_model = None\nbest_L1_reg = 0\nbest_L2_reg = 0\n\n# Todo 6:\nregs = [0.001, 0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040]\nregs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nfor type in range(2):\n    best_accuracy = 0\n    for i in range(len(regs)):\n        model = LinearRegression2()\n        reg = regs[i]\n        loss_history = model.train(X_train, Y_train, False, learning_rate, reg, reg_types[type])\n        accuracy = np.mean(model.predict(X_val) == Y_val)\n        if type == 0:\n            L1_reg_val_acc.append(accuracy)\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_L1_reg = reg\n                best_L1_model = model\n        else:\n            L2_reg_val_acc.append(accuracy)\n            if (accuracy > best_accuracy):\n                best_accuracy = accuracy\n                best_L2_reg = reg\n                best_L2_model = model\n        print(reg_types[type], reg, accuracy)\n\n#visuliza the relation of regularization parameter and validation accuracy\nx = range(len(regs))\nplt.plot(x, L1_reg_val_acc, label='L1_val_acc')\nplt.plot(x, L2_reg_val_acc, label='L2_val_acc')\nplt.xticks(x, regs)\nplt.margins(0.08)\nplt.legend()\nplt.xlabel('high-parameter reg')\nplt.ylabel('Validation Accuracy')\nplt.show()\n\n#Compute the performance of best model on the test set\nL1_pred = best_L1_model.predict(X_test)\nL1_acc = np.mean(L1_pred == Y_test)\nprint(\"The Accuracy with L1 regularization parameter {} is {}\\n\".format(best_L1_reg, L1_acc))\nL2_pred = best_L2_model.predict(X_test)\nL2_acc = np.mean(L2_pred == Y_test)\nprint(\"The Accuracy with L1 regularization parameter {} is {}\\n\".format(best_L2_reg, L2_acc))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "id": "EBA02A5D4B2147B583047207362FCAEC",
    "mdEditEnable": false
   },
   "source": "#### Question2: 对于上面的多分类逻辑回归模型，你觉得它的权重矩阵数值上会呈现出什么样子？你可以通过可视化的方法观察权重矩阵。\nAnswer2:\n\n可视化的结果在 “2.3 训练模型样例”里，为了使可视化结果明显，需要将正则化项调大到 `100~1000`。观察到权重矩阵可视化的结果分别是数字 0-9，在数字对应的区域值较大，在其他地方值比较小"
  },
  {
   "metadata": {
    "id": "DC024FC76A4C4F0CABB9B2A2A40F04CB",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}