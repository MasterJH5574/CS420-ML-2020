{
 "cells": [
  {
   "metadata": {
    "cell_type": "code",
    "id": "75705F0B6822407E9A895526DCBC9410",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "781E65565A47494FA3E19B454C1104E5",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_timesteps_input = 12\n",
    "num_timesteps_output = 12\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 50\n",
    "kernel_spatio = 3\n",
    "kernel_temporal = 3\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "667A7633C9034B37BDB0F30142E62029",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_adjacent_pickle(filename):\n",
    "    \"\"\"\n",
    "    :param filename: the filename of the pickle file\n",
    "    :return: ID_mapping: the mapping from node number in CSV to [0, num_nodes]\n",
    "             A: the normalized adjacency matrix\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = pk.load(f)\n",
    "    ID_mapping = data[0]\n",
    "    A = data[1]\n",
    "    cheb_A = [np.eye(A.shape[0]), A]\n",
    "    for i in range(2, kernel_spatio):\n",
    "        cheb_A.append(np.matmul(A * 2, cheb_A[-1]) - cheb_A[-2])\n",
    "    cheb_A = np.array(cheb_A)\n",
    "\n",
    "    return ID_mapping, cheb_A\n",
    "\n",
    "\n",
    "def load_csv(filename, ID_mapping):\n",
    "    \"\"\"\n",
    "    :param filename: the filename of the CSV file\n",
    "    :param ID_mapping: the mapping from node number in CSV to [0, num_nodes]\n",
    "    :return: data: the unnormalized np array in shape (num_nodes, num_timesteps, num_features=1)\n",
    "             num_nodes: number of nodes in graph\n",
    "             num_timesteps: number of timesteps\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    num_nodes = len(df.columns) - 1\n",
    "    num_timesteps = len(df)\n",
    "    data = [None] * num_nodes\n",
    "    time = df['timestamp'].tolist()\n",
    "    for i in range(len(time)):\n",
    "        time[i] = time[i][11:16].split(\":\")\n",
    "        time[i] = (int(time[i][0]) * 60 + int(time[i][1])) / 5\n",
    "\n",
    "    for (col_name, col_data) in df.iteritems():\n",
    "        if col_name == \"timestamp\":\n",
    "            continue\n",
    "        node_id = ID_mapping[col_name]\n",
    "        data[node_id] = col_data.values.tolist()\n",
    "        for i in range(num_timesteps):\n",
    "            #data[node_id][i] = [data[node_id][i], time[i]]\n",
    "            data[node_id][i] = [data[node_id][i]]\n",
    "\n",
    "    data = np.array(data)\n",
    "    return data, num_nodes, num_timesteps\n",
    "\n",
    "\n",
    "def modify_abnormal_data(unnormalized_data, num_nodes, num_timesteps):\n",
    "    \"\"\"\n",
    "    :param unnormalized_data: the unnormalized np array in shape (num_nodes, num_timesteps, num_features=1)\n",
    "    :param num_nodes: number of nodes in graph\n",
    "    :param num_timesteps: number of timesteps\n",
    "    :return: raw_data: the normalized data\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_timesteps):\n",
    "            #assert len(unnormalized_data[i][j]) == 2\n",
    "            assert len(unnormalized_data[i][j]) == 1\n",
    "            if abs(unnormalized_data[i][j][0]) <= 1e-6:\n",
    "                unnormalized_data[i][j][0] = (unnormalized_data[i][j - 1][0] + unnormalized_data[i][j + 1][0]) / 2\n",
    "\n",
    "    #mean = np.mean(unnormalized_data, axis=(0, 1))\n",
    "    #std = np.std(unnormalized_data, axis=(0, 1))\n",
    "    mean = np.mean(unnormalized_data)\n",
    "    std = np.std(unnormalized_data)\n",
    "    return unnormalized_data, mean, std\n",
    "\n",
    "\n",
    "def generate_training_data(raw_data, num_timesteps, mean, std):\n",
    "    X_train, Y_train = [], []\n",
    "\n",
    "    for time in range(num_timesteps - num_timesteps_input - num_timesteps_output + 1):\n",
    "        X_train.append(raw_data[:, time:time + num_timesteps_input])\n",
    "        #Y_train.append(raw_data[:, time + num_timesteps_input:time + num_timesteps_input + num_timesteps_output, 0])\n",
    "        Y_train.append(raw_data[:, time + num_timesteps_input:time + num_timesteps_input + num_timesteps_output])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "    X_train = (X_train - mean) / std\n",
    "    Y_train = (Y_train - mean) / std\n",
    "    #X_train[0] = (X_train[0] - mean[0]) / std[0]\n",
    "    #X_train[1] = (X_train[1] - mean[1]) / std[1]\n",
    "    #Y_train = (Y_train - mean[0]) / std[0]\n",
    "\n",
    "    Y_train = Y_train.reshape((Y_train.shape[0], Y_train.shape[1], Y_train.shape[2]))\n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "def generate_test_data(raw_data, mean, std):\n",
    "    X_test = []\n",
    "\n",
    "    for time in range(0, raw_data.shape[1], num_timesteps_output):\n",
    "        X_test.append(raw_data[:, time:time + num_timesteps_input])\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    #X_test[0] = (X_test[0] - mean[0]) / std[0]\n",
    "    #X_test[1] = (X_test[1] - mean[1]) / std[1]\n",
    "    X_test = (X_test - mean) / std\n",
    "    return X_test\n"
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "DD4272D7BF3D43BC89534800B26ADDEF",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ID_mapping, cheb_A = load_adjacent_pickle(\"adjacent.pkl\")\n",
    "unnormalized_data, num_nodes, num_timesteps = load_csv(\"train.csv\", ID_mapping)\n",
    "unnormalized_data, mean, std = modify_abnormal_data(unnormalized_data, num_nodes, num_timesteps)\n",
    "X_train, Y_train = generate_training_data(unnormalized_data, num_timesteps, mean, std)\n",
    "num_samples = X_train.shape[0]\n",
    "\n",
    "cheb_A = torch.from_numpy(cheb_A).double().to(device=device)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "Y_train = torch.from_numpy(Y_train)\n",
    "\n",
    "# shape of A: (num_nodes, num_nodes)\n",
    "# shape of cheb_A: (kernel_spatio, num_nodes, num_nodes)\n",
    "# shape of X_train and Y_train: (num_samples, num_nodes, num_timesteps_input/output, 1)\n",
    "\n",
    "#validation_split_line = int(X_train.shape[0] * 0.8)\n",
    "validation_split_line = int(X_train.shape[0] * 0.9)\n",
    "\n",
    "X_valid = X_train[validation_split_line:]\n",
    "Y_valid = Y_train[validation_split_line:]\n",
    "X_train = X_train[:validation_split_line]\n",
    "Y_train = Y_train[:validation_split_line]\n"
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "9BCBC36D5F764E819716759C8FD05850",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "unnormalized_data, _, _ = load_csv(\"test.csv\", ID_mapping)\n",
    "X_test = generate_test_data(unnormalized_data, mean, std)\n",
    "X_test = torch.from_numpy(X_test)\n"
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "36F3B0BD995E48F983C20A63E5081472",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Temporal_Gated_Conv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size):\n",
    "        super(Temporal_Gated_Conv, self).__init__()\n",
    "        self.output_channels = output_channels\n",
    "        #self.conv = nn.Conv2d(input_channels, 2 * output_channels, (1, kernel_size))\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels, (1, kernel_size))\n",
    "        self.conv2 = nn.Conv2d(input_channels, output_channels, (1, kernel_size))\n",
    "        #self.batch_norm = nn.BatchNorm2d(num_nodes)\n",
    "        self.layer_norm = nn.LayerNorm([num_nodes, output_channels])\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        :param X: input data with shape (batch_size, num_nodes, num_timesteps, input_channels)\n",
    "        :return: output data with shape (batch_size, num_nodes, num_timesteps(new), output_channels)\n",
    "        \"\"\"\n",
    "        # Step 1: convert X into shape (batch_size, num_channels, num_nodes, num_timesteps)\n",
    "        X = X.permute(0, 3, 1, 2)\n",
    "        # Step 2: perform convolution, split it into P and Q\n",
    "        #conv_result = self.conv(X)\n",
    "        #P = conv_result[:, :self.output_channels, :, :]\n",
    "        #Q = conv_result[:, self.output_channels:, :, :]\n",
    "        P = self.conv1(X)\n",
    "        Q = self.conv2(X)\n",
    "        # Step 3: calculate result, convert it into shape (batch_size, num_nodes, num_timesteps, num_channels)\n",
    "        result = P * torch.sigmoid(Q)\n",
    "        result = result.permute(0, 2, 3, 1)\n",
    "        #result = self.batch_norm(result)\n",
    "        result = self.layer_norm(result.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
    "        return result\n",
    "\n",
    "\n",
    "class Spatio_Graph_Conv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_spatio):\n",
    "        super(Spatio_Graph_Conv, self).__init__()\n",
    "        self.theta = nn.Parameter(torch.ones(()).new_empty((kernel_spatio, input_channels, output_channels),\n",
    "                                                           dtype=torch.float64))\n",
    "        #self.batch_norm = nn.BatchNorm2d(num_nodes)\n",
    "        self.layer_norm = nn.LayerNorm([num_nodes, output_channels])\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        lim = 1 / np.sqrt(self.theta.shape[2])\n",
    "        self.theta.data.uniform_(-lim, lim)\n",
    "\n",
    "    def forward(self, cheb_A, X):\n",
    "        \"\"\"\n",
    "        :param cheb_A: (kernel_spatio, num_nodes, num_nodes)\n",
    "        :param X: (batch_size, num_nodes, num_timesteps, input_channels)\n",
    "        :return: (batch_size, num_nodes, num_timesteps, output_channels)\n",
    "        \"\"\"\n",
    "        #mul_res1 = torch.einsum(\"mj,ijkl->imkl\", A, temporal_res1)\n",
    "        #mul_res2 = torch.einsum(\"imkl,ln->imkn\", mul_res1, self.theta)\n",
    "        mul_res1 = torch.einsum(\"knm,bmti->kbnti\", cheb_A, X)\n",
    "        mul_res2 = torch.einsum(\"kbnti,kio->bnto\", mul_res1, self.theta)\n",
    "        res3 = F.relu(mul_res2)\n",
    "        #res4 = self.batch_norm(res3)\n",
    "        res4 = self.layer_norm(res3.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
    "        return res4\n",
    "\n",
    "\n",
    "class ST_Conv_Block(nn.Module):\n",
    "    def __init__(self, input_channels, temporal_channels, spatio_channels, output_channels, temporal_kernel_size):\n",
    "        \"\"\"\n",
    "        :param input_channels: number of input channels\n",
    "        :param temporal_channels: number of output channels of the first temporal-gated-conv block\n",
    "        :param spatio_channels: number of output channels of the spatio graph-conv block\n",
    "        :param output_channels: number of output channels of the second temporal-gated-conv block\n",
    "        \"\"\"\n",
    "        super(ST_Conv_Block, self).__init__()\n",
    "        self.temporal_gated_conv1 = Temporal_Gated_Conv(input_channels, temporal_channels, temporal_kernel_size)\n",
    "        #self.theta = nn.Parameter(torch.ones(()).new_empty((temporal_channels, spatio_channels), dtype=torch.float64))\n",
    "        self.spatio_graph_conv = Spatio_Graph_Conv(temporal_channels, spatio_channels, kernel_spatio)\n",
    "        self.temporal_gated_conv2 = Temporal_Gated_Conv(spatio_channels, output_channels, temporal_kernel_size)\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        lim = np.sqrt(1 / self.theta.shape[1])\n",
    "        self.theta.data.uniform_(-lim, lim)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        :param A: the normalized spatio (n x n) matrix\n",
    "        :param X: input data with shape (batch_size, num_nodes, num_timesteps, input_channels)\n",
    "        :return: output data with shape (batch_size, num_nodes, num_timesteps, output_channels)\n",
    "        \"\"\"\n",
    "        temporal_res1 = self.temporal_gated_conv1(X)\n",
    "        spatio_res = self.spatio_graph_conv(cheb_A, temporal_res1)\n",
    "        #mul_res1 = torch.einsum(\"mj,ijkl->imkl\", A, temporal_res1)\n",
    "        #mul_res2 = torch.einsum(\"imkl,ln->imkn\", mul_res1, self.theta)\n",
    "        #spatio_res = F.relu(mul_res2)\n",
    "        #spatio_res = self.batch_norm(spatio_res)\n",
    "        temporal_res2 = self.temporal_gated_conv2(spatio_res)\n",
    "        return temporal_res2\n",
    "\n",
    "\n",
    "class STGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-Temporal Graph Convolutional Network\n",
    "    \"\"\"\n",
    "    def __init__(self, num_timesteps_input, num_timesteps_output, input_channels=1, temporal_kernel_size=3):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.st_conv_block1 = ST_Conv_Block(input_channels=input_channels, temporal_channels=64,\n",
    "                                            spatio_channels=16, output_channels=64,\n",
    "                                            temporal_kernel_size=temporal_kernel_size)\n",
    "        self.st_conv_block2 = ST_Conv_Block(input_channels=64, temporal_channels=64,\n",
    "                                            spatio_channels=16, output_channels=64,\n",
    "                                            temporal_kernel_size=temporal_kernel_size)\n",
    "        self.temporal_gated_conv = Temporal_Gated_Conv(input_channels=64, output_channels=64,\n",
    "                                                       kernel_size=temporal_kernel_size)\n",
    "        # shape (batch_size, num_nodes, num_timesteps (- (temporal_kernel_size - 1) * (2 + 2 + 1)), 64)\n",
    "        self.fully_connected = nn.Linear((num_timesteps_input - (temporal_kernel_size - 1) * 5) * 64,\n",
    "                                         num_timesteps_output)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        \"\"\"\n",
    "        :param A: the normalized spatio (n x n) matrix\n",
    "        :param X: input data with shape (batch_size, num_nodes, num_timesteps_input, input_channels=1)\n",
    "        :return: output data with shape (batch_size, num_nodes, num_timesteps_out)\n",
    "        \"\"\"\n",
    "        res1 = self.st_conv_block1(A, X)\n",
    "        res2 = self.st_conv_block2(A, res1)\n",
    "        res3 = self.temporal_gated_conv(res2)\n",
    "        res4 = self.fully_connected(res3.reshape((res3.shape[0], res3.shape[1], -1)))\n",
    "        return res4\n",
    "\n",
    "\n"
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "9A4C2FFE244C4809BF1269C770BF9C8E",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_stgcn(X_train, Y_train, batch_size=batch_size):\n",
    "    permutation = torch.randperm(X_train.shape[0])\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    #for t in range(0, validation_split_line, batch_size):\n",
    "    for t in range(0, X_train.shape[0], batch_size):\n",
    "        stgcn.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #indices = permutation[t:min(t + batch_size, validation_split_line)]\n",
    "        indices = permutation[t:min(t + batch_size, X_train.shape[0])]\n",
    "        X_batch, Y_batch = X_train[indices].to(device=device), Y_train[indices].to(device=device)\n",
    "\n",
    "        outputs = stgcn(cheb_A, X_batch)\n",
    "        loss = loss_function(outputs, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        stgcn.eval()\n",
    "\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "#def validate_stgcn(X_train, Y_train, batch_size=batch_size):\n",
    "def validate_stgcn(batch_size=batch_size):\n",
    "    #permutation = torch.randperm(X_train.shape[0])[validation_split_line:]\n",
    "    #X_validate = X_train[permutation]\n",
    "    #Y_validate = Y_train[permutation]\n",
    "\n",
    "    losses = []\n",
    "    rmse_losses = 0\n",
    "    #for t in range(0, X_validate.shape[0], batch_size):\n",
    "    for t in range(0, X_valid.shape[0], batch_size):\n",
    "        with torch.no_grad():\n",
    "            stgcn.eval()\n",
    "            #indices = range(t, min(t + batch_size, X_validate.shape[0]))\n",
    "            #X = X_validate[indices].to(device=device)\n",
    "            #Y = Y_validate[indices].to(device=device)\n",
    "            indices = range(t, min(t + batch_size, X_valid.shape[0]))\n",
    "            X = X_valid[indices].to(device=device)\n",
    "            Y = Y_valid[indices].to(device=device)\n",
    "            outputs = stgcn(cheb_A, X)\n",
    "            loss = loss_function(outputs, Y)\n",
    "            #rmse_mat = np.einsum(\"ijk,jmn->ijk\", outputs.cpu().numpy() - Y.cpu().numpy(), std)\n",
    "            #rmse_loss = np.sum(np.power(rmse_mat, 2))\n",
    "            #rmse_loss = np.sum(np.power((outputs.cpu().numpy() - Y.cpu().numpy()) * std[0], 2))\n",
    "            rmse_loss = np.sum(np.power((outputs.cpu().numpy() - Y.cpu().numpy()) * std, 2))\n",
    "            losses.append(loss.item())\n",
    "            rmse_losses += rmse_loss\n",
    "\n",
    "    #return sum(losses) / len(losses), np.sqrt(rmse_losses / np.size(Y_validate.cpu().numpy()))\n",
    "    return sum(losses) / len(losses), np.sqrt(rmse_losses / np.size(Y_valid.cpu().numpy()))\n",
    "\n",
    "\n",
    "def predict_stgcn(X_test, mean, std):\n",
    "    with torch.no_grad():\n",
    "        stgcn.eval()\n",
    "        X_test = X_test.to(device=device)\n",
    "        outputs = stgcn(cheb_A, X_test)\n",
    "    outputs = outputs.cpu().numpy() * std + mean\n",
    "    return outputs\n"
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "799CB3B177EA490C8A35917B7CE6C610",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def output_to_csv(data):\n",
    "    assert data.shape == (200, 325, 12)\n",
    "    data = np.moveaxis(data, 1, -1)\n",
    "    assert data.shape == (200, 12, 325)\n",
    "    str_out = \"ID,Prediction\\n\"\n",
    "    cnt = 0\n",
    "    for i in range(200): # test example\n",
    "        for j in range(12): # time steps\n",
    "            for k in range(325): # node number\n",
    "                str_out += '%d,%.5f\\n' % (cnt, data[i, j, k])\n",
    "                cnt += 1\n",
    "    with open('prediction.csv', 'w') as f:\n",
    "        f.write(str_out)\n"
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "id": "830701D4251442A7A91047C3E312015F",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "text": "start training\nepoch 0, training loss = 0.3252420264980251, validation loss = 0.25663652401334097, rmse = 4.8238272870010155\nepoch 1, training loss = 0.17895340993977318, validation loss = 0.1948960063383336, rmse = 4.202575532454292\nepoch 2, training loss = 0.15616843441578668, validation loss = 0.18782724007723442, rmse = 4.124250213599632\nepoch 3, training loss = 0.14547687300528492, validation loss = 0.17374735588212492, rmse = 3.967272920469077\nepoch 4, training loss = 0.1397879066614736, validation loss = 0.1649968536112403, rmse = 3.867263878234865\nepoch 5, training loss = 0.13317921538461575, validation loss = 0.16365752316524657, rmse = 3.8487741536452944\nepoch 6, training loss = 0.13102144029950935, validation loss = 0.15954349049115654, rmse = 3.803872804820111\nepoch 7, training loss = 0.12804644952018554, validation loss = 0.15696703781506818, rmse = 3.7739257827682855\nepoch 8, training loss = 0.12654186422617303, validation loss = 0.16333045173973929, rmse = 3.8463334347030464\nepoch 9, training loss = 0.12462796338811148, validation loss = 0.15427322987114128, rmse = 3.7424988597624287\nepoch 10, training loss = 0.12162004202509152, validation loss = 0.15315151739915484, rmse = 3.7286496290736717\nepoch 11, training loss = 0.12027390854179709, validation loss = 0.15360798231902054, rmse = 3.7347030901282827\nepoch 12, training loss = 0.11955238807034296, validation loss = 0.15685214778404133, rmse = 3.7748960859674643\nepoch 13, training loss = 0.1180218133148318, validation loss = 0.15116796597682874, rmse = 3.7049921996835846\nepoch 14, training loss = 0.11730049851391466, validation loss = 0.15511824649443728, rmse = 3.7535726723470844\nepoch 15, training loss = 0.1150621533908183, validation loss = 0.15176453195172473, rmse = 3.711642446141985\nepoch 16, training loss = 0.114210798559439, validation loss = 0.1584320618039393, rmse = 3.7916247675775443\nepoch 17, training loss = 0.11383970565569743, validation loss = 0.15225198575924473, rmse = 3.7194212418618062\nepoch 18, training loss = 0.11272699401041804, validation loss = 0.15401868191811263, rmse = 3.7421602406181584\nepoch 19, training loss = 0.1120624516881759, validation loss = 0.1532621119062703, rmse = 3.729353666479806\nepoch 20, training loss = 0.11062762756889785, validation loss = 0.14997022552282782, rmse = 3.68982322148184\nepoch 21, training loss = 0.10997237597005927, validation loss = 0.15125502881303918, rmse = 3.707079183805568\nepoch 22, training loss = 0.10965773772548687, validation loss = 0.14967896456548307, rmse = 3.686760124430957\nepoch 23, training loss = 0.10898367686457099, validation loss = 0.14986701781460057, rmse = 3.69004537328209\nepoch 24, training loss = 0.1082654598636461, validation loss = 0.15000663687603671, rmse = 3.6925186994497365\nepoch 25, training loss = 0.10752390744465007, validation loss = 0.151314953325489, rmse = 3.7089904973784638\nepoch 26, training loss = 0.10678400070775086, validation loss = 0.15113621448122097, rmse = 3.706719882310402\nepoch 27, training loss = 0.10650364483418373, validation loss = 0.15178874624906777, rmse = 3.7131225556427965\nepoch 28, training loss = 0.10593510650366343, validation loss = 0.15287360763026064, rmse = 3.726217479899481\nepoch 29, training loss = 0.10559424687716898, validation loss = 0.15137256181715567, rmse = 3.7079340354167742\nepoch 30, training loss = 0.10462747764955777, validation loss = 0.15217112200826388, rmse = 3.718241226251825\nepoch 31, training loss = 0.10432115613036358, validation loss = 0.15199288366942884, rmse = 3.713578742504189\nepoch 32, training loss = 0.10390491367377054, validation loss = 0.15308912532547714, rmse = 3.727938604965873\nepoch 33, training loss = 0.10365079962760274, validation loss = 0.15183333679049826, rmse = 3.7128770691687967\nepoch 34, training loss = 0.10341032397526027, validation loss = 0.15084779840726628, rmse = 3.701000669144061\nepoch 35, training loss = 0.10270320143344779, validation loss = 0.15095446957322456, rmse = 3.701520705339989\nepoch 36, training loss = 0.10249893480225566, validation loss = 0.1509235971582802, rmse = 3.702763133120154\nepoch 37, training loss = 0.10198962072887172, validation loss = 0.1513170587653491, rmse = 3.7063020096749044\nepoch 38, training loss = 0.10183904585811829, validation loss = 0.14950726123927965, rmse = 3.685637875494396\nepoch 39, training loss = 0.10158289862750008, validation loss = 0.15088990822485188, rmse = 3.7019509326003273\nepoch 40, training loss = 0.10109571855525275, validation loss = 0.1501682378668602, rmse = 3.6929523625261305\nepoch 41, training loss = 0.1007815111145519, validation loss = 0.15045468194219075, rmse = 3.6968276050296582\nepoch 42, training loss = 0.10069416568714505, validation loss = 0.1516936804754883, rmse = 3.7107711511477475\nepoch 43, training loss = 0.10046227239414701, validation loss = 0.1506969947110199, rmse = 3.6993392176547344\nepoch 44, training loss = 0.10026662897909802, validation loss = 0.1522954633341344, rmse = 3.7176874999924645\nepoch 45, training loss = 0.0997365740023254, validation loss = 0.1518730315126326, rmse = 3.7122659115947507\nepoch 46, training loss = 0.09955862844590435, validation loss = 0.15140806285170907, rmse = 3.707514346981327\nepoch 47, training loss = 0.09940314420037802, validation loss = 0.1521191002340078, rmse = 3.7162149464739227\nepoch 48, training loss = 0.09934304002164918, validation loss = 0.15117110660651742, rmse = 3.7052977820060953\nepoch 49, training loss = 0.0991092537040519, validation loss = 0.15211591739986316, rmse = 3.7157800349710906\n",
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/830701D4251442A7A91047C3E312015F/qaf2kvrihx.png\">"
     },
     "transient": {}
    },
    {
     "output_type": "display_data",
     "metadata": {
      "needs_background": "light"
     },
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "text/html": "<img src=\"https://cdn.kesci.com/rt_upload/830701D4251442A7A91047C3E312015F/qaf2kv93jl.png\">"
     },
     "transient": {}
    }
   ],
   "source": [
    "print(\"start training\")\n",
    "stgcn = STGCN(num_timesteps_input, num_timesteps_output, input_channels=1, temporal_kernel_size=3).double().to(device=device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(stgcn.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "rmse_losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss = train_stgcn(X_train, Y_train)\n",
    "    #validation_loss, rmse_loss = validate_stgcn(X_train, Y_train)\n",
    "    validation_loss, rmse_loss = validate_stgcn()\n",
    "    scheduler.step()\n",
    "\n",
    "    training_losses.append(loss)\n",
    "    validation_losses.append(validation_loss)\n",
    "    rmse_losses.append(rmse_loss)\n",
    "    print(\"epoch {}, training loss = {},\"\n",
    "          \" validation loss = {}, rmse = {}\".format(epoch, loss, validation_loss, rmse_loss))\n",
    "\n",
    "plt.plot(training_losses, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch Num\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rmse_losses, label=\"Validation RMSE Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch Num\")\n",
    "plt.ylabel(\"RMSE Loss\")\n",
    "plt.show()\n",
    "\n"
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "id": "B1DB0775A6774DEB8388BE6DBE4DD99B",
    "jupyter": {},
    "tags": [],
    "slideshow": {
     "slide_type": "slide"
    },
    "trusted": true,
    "collapsed": false,
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Y_test = predict_stgcn(X_test, mean, std)\n",
    "output_to_csv(Y_test)\n",
    "\n"
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}